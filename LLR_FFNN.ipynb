{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac65b2d4-0924-4f34-b149-4f2627a0ded9",
   "metadata": {},
   "source": [
    "# Load Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416a71a-5ba8-4cb9-8822-aa912aeff78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import remove_stopwords, split_on_space,strip_non_alphanum, strip_numeric,strip_punctuation,STOPWORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c343a9e-124a-4588-a49e-a70c15442f42",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dca162-31d4-40fc-bbdc-13065be98351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./dataset.csv', sep=',', encoding='unicode_escape')\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a3bf2-f460-4fe3-b875-24363bfa5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6315c-d961-40c9-a8d6-a2760ab8ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text2'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766dcab-d473-446f-8f37-80ad8d66136f",
   "metadata": {},
   "source": [
    "# process_LLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1543841-a1cd-4510-8719-261ce8d05bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_LLR(text):\n",
    "    # print(text)    \n",
    "    new_text = strip_punctuation(text.lower())\n",
    "    new_text = remove_stopwords(new_text)\n",
    "    new_text = strip_non_alphanum(new_text)\n",
    "    \n",
    "    all_stopwords_gensim = STOPWORDS.union(set(['s', 't','m','d']))\n",
    "    text_tokens = word_tokenize(new_text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "    new_text=(\" \").join(tokens_without_sw)\n",
    "    new_text = ' '.join(split_on_space(new_text))\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d6387-cc94-4085-8827-d27469bccbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:,\"LLR_text1\"] = df_train.loc[:,\"text1\"].apply(lambda x:process_LLR(x))\n",
    "df_train.loc[:,\"LLR_text2\"] = df_train.loc[:,\"text2\"].apply(lambda x:process_LLR(x))\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf163a9-4aef-4d69-9f0f-5bb395354744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text01 = df_train['LLR_text1'].tolist()\n",
    "text02 = df_train['LLR_text2'].tolist()\n",
    "labels = df_train['labels'].tolist() \n",
    "# print(text01,text02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428d592-ff4c-48b8-9cdf-04dfc4b884f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_data = []\n",
    "seg_data = []\n",
    "a=0\n",
    "for a in range(len(text01)):\n",
    "    if text01[a] != \" \":\n",
    "        text_01=text01[a]  \n",
    "        text_02=text02[a]\n",
    "        text_01_token = text_01.split(' ')\n",
    "        text_02_token = text_02.split(' ')\n",
    "        i=0\n",
    "        j=0\n",
    "        seg_data = []\n",
    "        seg_data.append(labels[a])\n",
    "        seg_data.append('\\t')\n",
    "        for i in range(len(text_01_token)):\n",
    "            for j in range(len(text_02_token)):\n",
    "\n",
    "                            # print(text01)\n",
    "                            # print(text01_token)\n",
    "                            # print(text02_token)\n",
    "                print(j)\n",
    "                texts=text_01_token[i]+\"-\"+text_02_token[j]\n",
    "                print(texts)\n",
    "                    # seg_data = []\n",
    "                    # seg_data.append(labels[i])\n",
    "                    # seg_data.append('\\t')\n",
    "                seg_data.append(texts)\n",
    "                seg = ''.join('%s' %id for id in seg_data)\n",
    "                seg_data.append(\" \")\n",
    "                print(seg)\n",
    "        final_data.append(seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ea4fc-31ae-4664-9cb2-f10b8f9c51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./20230626_ECC02_LLR.txt\",\"w\", encoding='UTF-8') as f:\n",
    "    for line in final_data:\n",
    "        f.write(line + os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515f5bf-320b-4a68-9c74-66e9c1202af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmunlp as nlp\n",
    "# What are the labels\n",
    "label_list=[\"2\",\"1\",\"0\"]  \n",
    "#file_path\n",
    "data_filepath = \"./20230626_ECC02_LLR.txt\"\n",
    "\n",
    "label_term_weighting = nlp.get_label_term_weighting(data_filepath, label_list)\n",
    "\n",
    "keyword_no_detected = nlp.get_keyword(\"0\", label_term_weighting, 2456, True)\n",
    "keyword_Support = nlp.get_keyword(\"1\", label_term_weighting, 2456, True)\n",
    "keyword_Attack = nlp.get_keyword(\"2\", label_term_weighting, 2456, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0853444-c6d7-4d6e-8fda-88b052b364aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keyword_no_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557648c-beba-46fb-9485-bc9c2a21bf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keyword_Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ce607-3043-46b9-9442-0c6b94f5d1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keyword_Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0c45c-9f5e-429d-8b03-215dc907e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR=pd.DataFrame(keyword_no_detected.items(), columns=['pair', 'score'])\n",
    "LLR.to_csv('./LLR_keyword_no_detected.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736aef05-c1b2-4c67-b3f5-c7ff340d8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR=pd.DataFrame(keyword_Support.items(), columns=['pair', 'score'])\n",
    "LLR.to_csv('./LLR_keyword_Support.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023feb13-1706-4d25-a463-e4b56fbfaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR=pd.DataFrame(keyword_Attack.items(), columns=['pair', 'score'])\n",
    "LLR.to_csv('./LLR_keyword_Attack.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1763c-1dd3-45b6-ae1c-79380389dba5",
   "metadata": {},
   "source": [
    "# LLR_PAIRED to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72645d6c-4f05-4d36-9a5d-c62fbdd24897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLR_keyword_no_detected = pd.read_csv('./LLR_keyword_no_detected.csv', encoding='UTF-8')\n",
    "print(LLR_keyword_no_detected)\n",
    "\n",
    "LLR_keyword_Support = pd.read_csv('./LLR_keyword_Support.csv', encoding='UTF-8')\n",
    "print(LLR_keyword_Support)\n",
    "\n",
    "LLR_keyword_Attack = pd.read_csv('./LLR_keyword_Attack.csv', encoding='UTF-8')\n",
    "print(LLR_keyword_Attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936ff14-f288-49f0-aeb7-95bd310d8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_keyword_no_detected['pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d818ea-c6cf-4147-92ff-debbe5db2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_keyword_Support['pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725a69e-6e23-4461-be03-3ce394d092cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_keyword_Attack['pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fbec9-1dd6-47b8-87b2-e57dec3fb7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('LLR02_keyword_no_detected.csv', 'w', encoding='UTF-8', newline='') as csvfile:\n",
    "    for i in range(2456):\n",
    "        words=LLR_keyword_no_detected['pair'][i].split('-')\n",
    "        print(words)\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(words)\n",
    "\n",
    "with open('LLR02_keyword_Support.csv', 'w', encoding='UTF-8', newline='') as csvfile:\n",
    "    for i in range(2456):\n",
    "        words=LLR_keyword_Support['pair'][i].split('-')\n",
    "        print(words)\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(words)\n",
    "        \n",
    "\n",
    "with open('LLR02_keyword_Attack.csv', 'w', encoding='UTF-8', newline='') as csvfile:\n",
    "    for i in range(2456):\n",
    "        words=LLR_keyword_Attack['pair'][i].split('-')\n",
    "        print(words)\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b372de6-8996-47ef-9dd2-2edb6800bb24",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc11214-8d8d-441c-8cca-984c590b4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./dataset.csv', sep=',', encoding='unicode_escape')\n",
    "df_train.head(2)\n",
    "df_dev = pd.read_csv('./ECCAR.csv', sep=',', encoding='unicode_escape')\n",
    "df_dev.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fac5a3-f106-4a28-8281-e14a11198d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_keyword_no_detected = pd.read_csv('./LLR02_keyword_no_detected.csv', sep=',', encoding='unicode_escape')\n",
    "print(df_keyword_no_detected.head(2))\n",
    "\n",
    "df_keyword_Support = pd.read_csv('./LLR02_keyword_Support.csv', sep=',', encoding='unicode_escape')\n",
    "print(df_keyword_Support.head(2))\n",
    "\n",
    "df_keyword_Attack = pd.read_csv('./LLR02_keyword_Attack.csv', sep=',', encoding='unicode_escape')\n",
    "print(df_keyword_Attack.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57058fc6-3e68-4d89-9ad9-a04d80747ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword_no_detected['text1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a2728-ecd0-4312-8056-40887974f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c57c1-deda-4ff6-abd7-83d644f84ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "df_train.loc[:,\"LLR_text1\"] = df_train.loc[:,\"text1\"].apply(lambda x:process_LLR(x))\n",
    "df_train.loc[:,\"LLR_text2\"] = df_train.loc[:,\"text2\"].apply(lambda x:process_LLR(x))\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa68a6-d0f4-4baa-bd16-21d8401f8f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text01 = df_train['LLR_text1'].tolist()\n",
    "print(text01)\n",
    "text02 = df_train['LLR_text2'].tolist()\n",
    "print(text02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc4b32-d996-4cf8-8c46-f2b04ce5d31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLR_answer=pd.DataFrame()\n",
    "\n",
    "# 重複3個for\n",
    "for i in range(500):\n",
    "    aList=[]\n",
    "    for j in range(5521):\n",
    "        think_found = df_keyword_no_detected['text1'][i] in df_train['LLR_text1'][j]\n",
    "        # print(df_train['text1'][j])\n",
    "        # print(df_keyword['text1'][0])\n",
    "        # print(think_found)\n",
    "        sales_found = df_keyword_no_detected['text2'][i] in df_train['LLR_text2'][j]\n",
    "        # print(df_train['text2'][j])\n",
    "        # print(df_keyword['text2'][0])\n",
    "        # print(sales_found)\n",
    "        item_b=0\n",
    "        if think_found and sales_found:\n",
    "            # print(\"18.78954419\")\n",
    "            item=1\n",
    "        # item_b=item_b+item\n",
    "            item_b=df_keyword_no_detected['score'][i]\n",
    "        aList.extend([item_b])\n",
    "    answer=aList\n",
    "    # print(answer)\n",
    "    LLR_answer.insert(0,i, answer, True)\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(300):\n",
    "    aList=[]\n",
    "    for j in range(5521):\n",
    "        think_found = df_keyword_Support['text1'][i] in df_train['LLR_text1'][j]\n",
    "        # print(df_train['text1'][j])\n",
    "        # print(df_keyword['text1'][0])\n",
    "        # print(think_found)\n",
    "        sales_found = df_keyword_Support['text2'][i] in df_train['LLR_text2'][j]\n",
    "        # print(df_train['text2'][j])\n",
    "        # print(df_keyword['text2'][0])\n",
    "        # print(sales_found)\n",
    "        item_b=0\n",
    "        if think_found and sales_found:\n",
    "            # print(\"18.78954419\")\n",
    "            item=1\n",
    "            # item_b=item_b+item\n",
    "            item_b=df_keyword_Support['score'][i]\n",
    "        aList.extend([item_b])\n",
    "    answer=aList\n",
    "    # print(answer)\n",
    "    LLR_answer.insert(0,i+300, answer, True)\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(2000):\n",
    "    aList=[]\n",
    "    for j in range(5521):\n",
    "        think_found = df_keyword_Attack['text1'][i] in df_train['LLR_text1'][j]\n",
    "        # print(df_train['text1'][j])\n",
    "        # print(df_keyword['text1'][0])\n",
    "        # print(think_found)\n",
    "        sales_found = df_keyword_Attack['text2'][i] in df_train['LLR_text2'][j]\n",
    "        # print(df_train['text2'][j])\n",
    "        # print(df_keyword['text2'][0])\n",
    "        # print(sales_found)\n",
    "        item_b=0\n",
    "        if think_found and sales_found:\n",
    "            # print(\"18.78954419\")\n",
    "            item=1\n",
    "        # item_b=item_b+item\n",
    "            item_b=df_keyword_Attack['score'][i]\n",
    "        aList.extend([item_b])\n",
    "    answer=aList\n",
    "    # print(answer)\n",
    "    LLR_answer.insert(0,i+2300, answer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209ae49-52ae-458a-b4b5-139b07b36ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38d1ea-1213-4d59-8819-5b4ee52a035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49111278-3da4-46ab-bee1-c16ca61e3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_answer.insert(0,\"labels\", df_train['labels'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0852595-fd34-4403-9e68-d3d89f85e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f1e70-58b0-4243-b35a-502922aff9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['sentence1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fcbad-7ed7-4f05-a8a4-91956ac84b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev\n",
    "from gensim.parsing.preprocessing import remove_stopwords, split_on_space,strip_non_alphanum, strip_numeric,strip_punctuation,STOPWORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def process_LLR(text):\n",
    "    # print(text)    \n",
    "    new_text = strip_punctuation(text.lower())\n",
    "    new_text = remove_stopwords(new_text)\n",
    "    new_text = strip_non_alphanum(new_text)\n",
    "    \n",
    "    all_stopwords_gensim = STOPWORDS.union(set(['s', 't','m','d']))\n",
    "    text_tokens = word_tokenize(new_text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "    new_text=(\" \").join(tokens_without_sw)\n",
    "    new_text = ' '.join(split_on_space(new_text))\n",
    "    return new_text\n",
    "\n",
    "\n",
    "df_dev.loc[:,\"LLR_text1\"] = df_dev.loc[:,\"sentence1\"].apply(lambda x:process_LLR(x))\n",
    "df_dev.loc[:,\"LLR_text2\"] = df_dev.loc[:,\"sentence2\"].apply(lambda x:process_LLR(x))\n",
    "df_dev.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c0afa-cca1-4cf7-b9af-9a1342a39a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "text01 = df_dev['LLR_text1'].tolist()\n",
    "print(text01)\n",
    "\n",
    "text02 = df_dev['LLR_text2'].tolist()\n",
    "print(text02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e691a49-4187-4523-89a5-dc2e2a3fe4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLR_answer_dev=pd.DataFrame()\n",
    "\n",
    "# 重複3個for\n",
    "for i in range(500):\n",
    "    aList=[]\n",
    "    for j in range(690):\n",
    "        think_found = df_keyword_no_detected['text1'][i] in df_dev['LLR_text1'][j]\n",
    "        # print(df_train['text1'][j])\n",
    "        # print(df_keyword['text1'][0])\n",
    "        # print(think_found)\n",
    "        sales_found = df_keyword_no_detected['text2'][i] in df_dev['LLR_text2'][j]\n",
    "        # print(df_train['text2'][j])\n",
    "        # print(df_keyword['text2'][0])\n",
    "        # print(sales_found)\n",
    "        item_b=0\n",
    "        if think_found and sales_found:\n",
    "            # print(\"18.78954419\")\n",
    "            item=1\n",
    "        # item_b=item_b+item\n",
    "            item_b=df_keyword_no_detected['score'][i]\n",
    "        aList.extend([item_b])\n",
    "    answer=aList\n",
    "    # print(answer)\n",
    "    LLR_answer_dev.insert(0,i, answer, True)\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(300):\n",
    "    aList=[]\n",
    "    for j in range(690):\n",
    "        think_found = df_keyword_Support['text1'][i] in df_dev['LLR_text1'][j]\n",
    "        # print(df_train['text1'][j])\n",
    "        # print(df_keyword['text1'][0])\n",
    "        # print(think_found)\n",
    "        sales_found = df_keyword_Support['text2'][i] in df_dev['LLR_text2'][j]\n",
    "        # print(df_train['text2'][j])\n",
    "        # print(df_keyword['text2'][0])\n",
    "        # print(sales_found)\n",
    "        item_b=0\n",
    "        if think_found and sales_found:\n",
    "            # print(\"18.78954419\")\n",
    "            item=1\n",
    "            # item_b=item_b+item\n",
    "            item_b=df_keyword_Support['score'][i]\n",
    "        aList.extend([item_b])\n",
    "    answer=aList\n",
    "    # print(answer)\n",
    "    LLR_answer_dev.insert(0,i+300, answer, True)\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(2000):\n",
    "    aList=[]\n",
    "    for j in range(690):\n",
    "        think_found = df_keyword_Attack['text1'][i] in df_dev['LLR_text1'][j]\n",
    "        # print(df_train['text1'][j])\n",
    "        # print(df_keyword['text1'][0])\n",
    "        # print(think_found)\n",
    "        sales_found = df_keyword_Attack['text2'][i] in df_dev['LLR_text2'][j]\n",
    "        # print(df_train['text2'][j])\n",
    "        # print(df_keyword['text2'][0])\n",
    "        # print(sales_found)\n",
    "        item_b=0\n",
    "        if think_found and sales_found:\n",
    "            # print(\"18.78954419\")\n",
    "            item=1\n",
    "        # item_b=item_b+item\n",
    "            item_b=df_keyword_Attack['score'][i]\n",
    "        aList.extend([item_b])\n",
    "    answer=aList\n",
    "    # print(answer)\n",
    "    LLR_answer_dev.insert(0,i+2300, answer, True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487401f-c00e-4aa2-8ea3-242e17840bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev['labels']\n",
    "df_dev['Prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c543a-5e76-4e9e-a20b-23f16b9bcd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLR_answer_dev.insert(0,\"labels\", df_dev['Prediction'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d9311-9e5e-4b2a-90bd-a9ba60a9be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_answer_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4835cac-712a-47f1-ab15-8a1c80ebbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875dd59-27e5-49eb-baf5-7a8b14a4ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LLR_answer['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66cae5-b38c-49cb-ba76-826dfbc6dafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_dev = LLR_answer_dev['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92b7c8-6267-4ec2-96ef-0631578d2a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# i=3\n",
    "# # LLR_answer=pd.DataFrame()\n",
    "# answer=aList\n",
    "# LLR_answer.insert(0,i, answer, True)\n",
    "# print(LLR_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54aed9-468b-4f72-8d38-a19ea2556095",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=LLR_answer.drop(['labels'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912e43f-c9b9-44b8-af9e-801e11de9c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_dev=LLR_answer_dev.drop(['labels'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed2e98-1c85-426c-9097-59a0fca02df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34acb9a3-1c37-4180-8e78-e764dbdb1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_answer_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb30275-9e6d-4547-90f9-4750d313087c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430fad0-b5e2-4471-ae3f-f2b22661c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input, concatenate\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from attention import Attention\n",
    "\n",
    "#features欄位數\n",
    "m_height = 1500\n",
    "#label欄位數\n",
    "m_width = 1\n",
    "\n",
    "def get_dnn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "    return model\n",
    "\n",
    "def get_CLSTM():\n",
    "    x_input = Input(shape=(m_height, m_width))\n",
    "    cnn = Conv1D(filters=28, #30\n",
    "                 kernel_size=2, #4\n",
    "                 strides=1, #1\n",
    "                 padding=\"valid\", #valid\n",
    "                 data_format=\"channels_last\", #channels_last\n",
    "                 dilation_rate=1, #1\n",
    "                 groups=1, #1\n",
    "                 activation=None, #None\n",
    "                 use_bias=True, #True\n",
    "                 kernel_initializer=\"glorot_uniform\", #glorot_uniform\n",
    "                 bias_initializer=\"zeros\", #zeros\n",
    "                 kernel_regularizer=None, #None\n",
    "                 bias_regularizer=None, #None\n",
    "                 activity_regularizer=None, #None\n",
    "                 kernel_constraint=None, #None\n",
    "                 bias_constraint=None)(x_input) #None\n",
    "    \n",
    "##(pool_size=int(cnn.shape[1])) 未進行使用，詢問老師是否重要\n",
    "    cnn = MaxPooling1D(pool_size=3,strides=3,padding=\"valid\",data_format=\"channels_last\")(cnn)\n",
    "    # cnn = Flatten()(cnn)\n",
    "    cnn = Dense(24)(cnn)\n",
    "    f = LSTM(units=12)(cnn)\n",
    "    y = Dense(3, activation='softmax')(f)\n",
    "    return Model([x_input], outputs=[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584a285-471b-4e8f-af4d-5c86dab900f0",
   "metadata": {},
   "source": [
    "80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7ddbc-ea8d-44ee-b878-b7873d0a6140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "callbacks_list = [\n",
    "    EarlyStopping(verbose=True, patience=5, monitor='val_accuracy'),\n",
    "    ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "]\n",
    "\n",
    "predicted=[]\n",
    "expected=[]\n",
    "predicted01=[]\n",
    "\n",
    "x_train=features\n",
    "x_test=features_dev\n",
    "y_train=label\n",
    "y_test=label_dev\n",
    "# x_train,x_test,y_train,y_test = train_test_split(features, label, test_size=0.2, stratify=label, random_state=42) \n",
    "model = get_dnn()\n",
    "class_weights = {0: 2.5,1:1 ,2: 80.25}\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(1e-3), metrics = ['accuracy']) #1e-2,2e-3\n",
    "# x_train_rs, y_train_rs = SMOTE().fit_resample(x_train, y_train) \n",
    "model.fit(x_train, utils.to_categorical(y_train), batch_size = 32, epochs = 50,  #16 10\n",
    "        class_weight=class_weights,\n",
    "        verbose = 2, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "result=model.predict(x_test)\n",
    "expected.extend(y_test)\n",
    "predicted.extend(model.predict(x_test).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3213b41-2cf5-4e76-bb26-d11d3ece2662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d09f4-1b33-4275-88de-296b64eb4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba4aa6-4dcc-49e3-a133-9fc8747e48d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "df.insert(0,\"labels\", y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1318d14-b1a2-4e3e-9f14-5381d97974a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506db2fe-8749-442a-93ff-15cf05feda0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('LLRDNN_result_classweight.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515a8be-b1ee-4def-95b4-7d71881e276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "classes_= ['No detected', 'Support', 'Attack']\n",
    "print(metrics.classification_report(expected, predicted, target_names =classes_))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print(\"Macro f1-score: \"+ str(f1_score(expected, predicted, average='macro')))\n",
    "print(\"Micro f1-score: \"+ str(f1_score(expected, predicted, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5f892-bb73-4e7d-a2fb-54926f4457df",
   "metadata": {},
   "source": [
    "10-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df06421-028f-4f54-bd1e-025be85f931b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#without resampling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "callbacks_list = [\n",
    "    EarlyStopping(verbose=True, patience=5, monitor='val_accuracy'),\n",
    "    ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "]\n",
    "\n",
    "predicted=[]\n",
    "expected=[]\n",
    "\n",
    "\n",
    "for train_index, test_index in KFold(n_splits=10, shuffle=True).split(features):\n",
    "    x_train=np.array(features)[train_index]\n",
    "    y_train=np.array(label)[train_index]\n",
    "    x_test=np.array(features)[test_index]\n",
    "    y_test=np.array(label)[test_index]\n",
    "    \n",
    "    model = get_dnn()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam', metrics = ['accuracy'])\n",
    "    class_weights = {0: 5,1:1 ,2: 10}\n",
    "    \n",
    "    model.fit(x_train, utils.to_categorical(y_train), batch_size = 32, epochs = 50,  #16 10\n",
    "              class_weight=class_weights,\n",
    "        verbose = 2, validation_split=0.2, callbacks=callbacks_list)\n",
    "    \n",
    "\n",
    "    model.load_weights(\"weights.best.hdf5\")\n",
    "    expected.extend(y_test)\n",
    "    predicted.extend(model.predict(x_test).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd586757-b251-4216-9a43-0cc817f9f673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with resampling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "callbacks_list = [\n",
    "    EarlyStopping(verbose=True, patience=5, monitor='val_accuracy'),\n",
    "    ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "]\n",
    "\n",
    "predicted=[]\n",
    "expected=[]\n",
    "\n",
    "\n",
    "for train_index, test_index in KFold(n_splits=10, shuffle=True).split(features):\n",
    "    x_train=np.array(features)[train_index]\n",
    "    y_train=np.array(label)[train_index]\n",
    "    x_test=np.array(features)[test_index]\n",
    "    y_test=np.array(label)[test_index]\n",
    "    \n",
    "    x_train_rs, y_train_rs = SMOTE().fit_resample(x_train, y_train)\n",
    "    model = get_dnn()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    model.fit(x_train_rs, utils.to_categorical(y_train_rs), batch_size = 32, epochs = 50,  #16 10\n",
    "        verbose = 2, validation_split=0.2, callbacks=callbacks_list)\n",
    "    \n",
    "\n",
    "    model.load_weights(\"weights.best.hdf5\")\n",
    "    expected.extend(y_test)\n",
    "    predicted.extend(model.predict(x_test).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8add31-e99a-44cb-9ae3-c7daee763feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "classes_= ['No detected', 'Support', 'Attack']\n",
    "print(metrics.classification_report(expected, predicted, target_names =classes_))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print(\"Macro f1-score: \"+ str(f1_score(expected, predicted, average='macro')))\n",
    "print(\"Micro f1-score: \"+ str(f1_score(expected, predicted, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b3d13-5cc1-4bb4-adc5-ae4456debfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "classes_= ['No detected', 'Support', 'Attack']\n",
    "print(metrics.classification_report(expected, predicted, target_names =classes_))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print(\"Macro f1-score: \"+ str(f1_score(expected, predicted, average='macro')))\n",
    "print(\"Micro f1-score: \"+ str(f1_score(expected, predicted, average='micro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
